% 第四章：GUI
\chapter{面向用户图形界面的视觉语言模型能力提升方案}
\label{chap:gui}

\section{背景}
随着多模态大语言模型（Multimodal Large Language Model, MLLM）在通用视觉理解任务上的快速突破，面向真实设备自动化的\textbf{UI Agent}（交互界面代理）逐渐成为落地应用的重要方向。与通用场景不同，移动端与桌面端GUI具有\textbf{控件密集、文字细小、布局结构化、交互状态易变}等特征，使得模型在“看懂屏幕”这一前置能力上面临更高门槛。

而在UI Agent的感知--决策闭环中，\textbf{Referring（指代识别）}与\textbf{Grounding（定位/落地）}是最基础也最关键的两项能力：
% 之后要加入图片示例
\begin{itemize}
  \item \textbf{Referring}：给定屏幕区域的边界框（Bounding Box, bbox），模型需要输出该区域控件的功能、文本或属性描述，即模型的理解界面能力，例如输入“在bbox [10,10,50,50]区域的控件是什么？”，输出“这是搜索按钮”。
  \item \textbf{Grounding}：给定自然语言指令，模型需要在屏幕截图中定位对应控件并输出其bbox，即模型定位的能力，例如输入“登陆按钮在哪里？”，输出“bbox [100,200,300,400]”。
\end{itemize}

尽管通用MLLM已具备一定视觉理解能力，但在GUI垂直领域仍存在三个主要挑战：\textbf{（1）细粒度定位偏差}，小面积控件的坐标难以稳定。\textbf{（2）图片信息损失严重}，直接缩放图片后细节与文字信息丢失严重。\textbf{（3）视觉与语言难以对齐}，传统检测模型缺乏语义理解能力、视觉语言模型定位效果存在偏差）。为此，本章提出一套以\textbf{多源数据工程}为核心、结合\textbf{双阶段混合微调（DMT）}与\textbf{高分辨率架构优化}的系统方案，目标是在移动端GUI最流行前200应用的使用场景测试数据中实现Referring准确率$\ge 95\%$、单控件Grounding准确率$\ge 95\%$。

\section{数据工程：多源融合与高质量构建}
数据是GUI理解能力提升的基石。本节围绕“\textbf{多源融合}”与“\textbf{高质量构建}”两条主线，构建开源清洗数据、闭源精标数据与合成数据相互补位的训练语料体系，以保证数据质量可控、可迭代。

\subsection{GUI数据构造流程}
GUI领域的Referring与Grounding能力提升，本质依赖于“\textbf{理解语义}且\textbf{精准定位}”的数据监督：模型既要学会控件的语义功能（登录/搜索/关闭等），又要学会在高分辨率、密集布局中输出像素级稳定的定位结果。相比自然图像，GUI场景存在\textbf{小目标密集}、\textbf{文字依赖强}与\textbf{交互状态多变}等特点，其中“文字依赖”指的是：移动端控件\textbf{种类多样、外观相似但功能多变}，大量关键语义（如“登录/下一步/同意/关闭”以及输入框的字段含义与提示信息）主要通过界面文字表达，必须借助截图中的文本信息（OCR）才能准确判断控件身份与动作目标。使得简单的图文对或粗粒度检测标注难以覆盖真实需求。因此，为了切实提升模型的GUI场景下的能力，本节提出了一种可以处理多种开源数据，同时通过重写、人工精标的方法来构建高质量数据的数据构造流程。

整体流程可概括为“\textbf{收集多种源数据$\rightarrow$结构化初始化$\rightarrow$混合标注与属性补全$\rightarrow$清洗与结构化聚合$\rightarrow$任务/QA数据生成与过滤$\rightarrow$质量控制与迭代}”，其必要性体现在以下几个方面：
\begin{itemize}
  \item \textbf{收集多种源数据}：为覆盖真实分布与长尾场景，数据来源需同时包含开源GUI样本（提供多样布局先验）、闭源真实App截图（补齐中文与主流应用生态）、以及必要的合成/重写样本（补足稀缺交互形态与困难指令表达）。这一阶段决定了后续数据分布的上限。
  \item \textbf{结构化初始化}：将不同来源样本统一为可计算、可追溯的结构化记录（截图、分辨率、视图层级、候选控件集合等），并规范坐标表示与字段定义。该步骤的作用是把“图像”转化为可被标注系统与训练系统共同消费的中间表示，为后续的类别映射、关系建模与OCR对齐提供稳定接口。
  \item \textbf{混合标注与属性补全}：围绕控件级bbox与文字内容进行高质量标注，并补充可交互性与状态等属性。该步骤通常采用“检测+OCR+属性预测”的自动化预标注，并由人工校验与补充兜底。其作用是同时回应GUI三大难点：通过紧致bbox缓解小控件定位偏差，通过OCR字段解决文字依赖，通过状态/可交互标签避免“看似可点但不可点”的误监督。
  \item \textbf{任务/QA数据生成与过滤}：在结构化标注与控件集合基础上生成面向训练的指令/问答数据。一方面将能力拆解为可监督子任务：\textbf{感知类}（定位/识别/OCR）、\textbf{理解类}（控件语义解释与指代描述）、\textbf{推理类}（结合上下文的动作选择与多步计划）；另一方面可进一步生成描述UI功能语义与操作意图的QA数据，并通过规则过滤与相似度去重降低噪声与冗余。该步骤的关键是把“标注信息”组织成模型可学习的交互格式，从而同时对齐语义与布局两条能力链路。
  \item \textbf{质量控制与迭代}：对样本进行一致性与正确性检查（例如bbox合法性、OCR与文本一致性、指令与目标控件匹配、关系图完整性），并将失败样本回收为后续补标/重写/再采集的候选。该步骤是保证数据分布稳定、降低噪声与抑制幻觉的必要条件。
\end{itemize}

需要强调的是，上述流程虽然并非后续章节提出的多模态数据合成范式，而是从GUI任务需求出发的一种可行数据工程路径：它首先明确了GUI训练数据必须同时覆盖\textbf{语义}与\textbf{图形定位}，并以“结构化初始化--精细标注--任务化构造--质控迭代”为核心步骤，为后续提出更通用的多模态数据构造框架奠定基础。

\subsection{开源数据清洗与重构}
开源数据在规模与多样性上具备优势，但往往存在噪声、布局失真与语义标签不一致的问题。以RICO为代表的GUI数据集包含大量移动端界面及其视图层级（View Hierarchy），然而原始数据中存在不可见节点、错误标注、层级断裂等现象，直接用于训练会导致模型学习到错误的几何与语义先验。本文采用CLAY（Clean Layout for Android UI）思路对RICO进行清洗与增强，主要包括：
\begin{itemize}
  \item \textbf{噪声过滤与层级修复}：剔除包含“INVALID”标签、尺寸为0、透明不可见等节点，并修复视图树结构，确保层级关系可用于后续关系建模与样本构造。
  \item \textbf{语义标签映射}：将原始Android View类名映射为更具语义的通用组件类别，以减少类别碎片化带来的学习难度，例如将\texttt{android.widget.\allowbreak EditText}及其子类统一映射为\texttt{TEXT\_INPUT}，将\texttt{FloatingActionButton}映射为\texttt{BUTTON}。
  \item \textbf{任务Prompt构造}：在清洗后的结构化数据上生成标准化监督样本，包括：
    \begin{itemize}
      \item \textbf{Grounding}：输入自然语言指令（如“点击右上角的搜索图标”）与页面截图，输出归一化bbox；
      \item \textbf{Referring}：输入bbox与页面截图，输出组件类别、功能与可见文本（可结合OCR字段）。
    \end{itemize}
\end{itemize}
通过上述处理，开源数据为模型提供了覆盖广的GUI布局先验与基础交互语义，但在中文场景、复杂交互与状态理解上仍存在缺口，需要闭源数据与合成数据补齐。

除RICO外，本文在开源侧还引入了\textbf{MobileViews}与\textbf{OS-Atlas}两类GUI数据，以补齐交互覆盖与定位监督：
\begin{itemize}
  \item \textbf{MobileViews}：MobileViews是面向移动端智能体与UI分析的大规模数据集，其MobileViews-600K版本包含从Google Play Store上两万余应用采集的60万级“屏幕截图--视图层次结构（VH）”配对数据。其最新版本基于DroidBot并针对大规模采集进行了优化，在保持与DroidBot输出结构一致的同时，能够捕获更全面的交互细节。本文利用该数据集中控件的\textbf{类型}与\textbf{组成信息}（包括bbox与控件内文字）构造训练样本：一方面将控件文字作为OCR监督与Referring依据，另一方面将VH中的层级与控件类型作为语义先验，增强模型对复杂页面结构的泛化能力。
  \item \textbf{OS-Atlas}：本文主要使用OS-Atlas提供的GUI Grounding数据集，其中目标元素位置存储在\texttt{bbox}字段，采用$[left,\,top,\,right,\,bottom]$的归一化小数表示（每个值均为$[0,1]$范围内相对图像宽高的比例）。在任务构造时，我们将其与其它数据源统一到一致的坐标表示与字段规范（并保持bbox的几何语义一致），用于补强“指令$\rightarrow$目标控件位置”的Grounding监督，尤其覆盖长尾指令表述与跨应用布局差异。
\end{itemize}

\subsection{闭源数据构建}
针对开源数据在中文语境、主流App生态与复杂交互状态覆盖上的不足，本文构建闭源高质量GUI数据，并按迭代版本记为\textbf{v1}与\textbf{v2}：其中v1用于早期验证与规范冷启动，重点覆盖文本输入、开关、弹窗等高频交互组件；v2在v1基础上进行二次校验，剔除定义不清的控件并加入\textbf{负样本}（不可点击区域/误导性区域），以提升Grounding的判别能力。闭源数据构建强调“\textbf{可训练}”与“\textbf{可评测}”的一致性：既提供控件级几何定位监督，也提供可解释的语义与状态监督，并进一步产出可用于指令学习的UI意图问答数据。

\paragraph{第一阶段：数据标注（混合标注模式）}
该阶段从原始截图开始（可选携带页面XML/VH等元数据），采用“自动化预标注+人工校验补充+纯人工兜底”的混合模式，以兼顾效率与准确性：
\begin{itemize}
  \item \textbf{UI元素检测}：使用轻量检测模型（如YOLO）自动检测图标、按钮、输入框等控件并输出bbox。
  \item \textbf{文字识别（OCR）}：对候选框区域使用PaddleOCR提取控件文字（如“登录”“取消”等），形成文字与位置的对齐监督。
  \item \textbf{属性预测}：利用已训练的多模态模型（如qwen2vl）对候选控件进行属性补全，例如控件细分类别、可交互性以及选中/开启等状态。
  \item \textbf{人工介入与整合}：标注人员对预标注结果进行校验与修正，并补充模型遗漏的控件；对于复杂样本走纯人工标注分支。最终合并得到统一标注结果，确保每个关键控件的\textbf{BBox（位置）}、\textbf{类别（类型）}与\textbf{属性（文字/状态/可用性等）}准确一致。
\end{itemize}

\textbf{OCR增强}是闭源数据的重要补强环节。为降低通用OCR在艺术字体与复杂背景下的误识别与“幻觉”，本文引入更高精度的文本抽取与校验流程，生成用于训练的高质量中文文本标注，从而让模型在Referring任务中能够稳定对齐“控件外观--文字内容--交互语义”三者。

\paragraph{第二阶段：数据清洗与结构化}
该阶段对标注结果进行质量控制与格式转换，生成训练友好的结构化数据表示：
\begin{itemize}
  \item \textbf{质量校验}：检查bbox合法性、OCR文本异常（如过长/乱码）、属性冲突（例如“不可交互但标为可点击”）以及控件间的关联关系是否合理。
  \item \textbf{数据提纯与聚合}：生成包含控件类型、属性、bbox的纯净样本，并以图片为单位聚合为控件列表（每张截图关联一个包含其所有UI元素的集合）。
  \item \textbf{格式转换}：将结构化控件集合转换为多种下游格式，以支持指代理解（ref）、定位落地（grd）、列表/容器理解（list）等任务的数据构造与训练。
\end{itemize}

\paragraph{第三阶段：QA数据生成与过滤}
该阶段是将结构化控件信息转化为“可学习的功能语义与操作意图”的核心步骤：将聚合好的控件信息与精心设计的系统提示（sys prompt）及少样例（few-shot examples）结合，调用强多模态模型（如gpt4o/qwen2vl）生成关于界面功能与操作意图的问答对（QA）。例如，生成问题：“如何退出登录？”并给出可执行的操作序列描述。
为保证质量与覆盖，进一步执行数据过滤与去重：
\begin{itemize}
  \item \textbf{过滤噪音}：基于规则（如问题过短、答案无意义、指令与目标控件不匹配等）过滤低质量样本；
  \item \textbf{去重降冗余}：计算图片表征向量（image embedding），基于相似度剔除视觉高度相似页面上生成的重复或冗余QA，避免训练集冗余堆积。
\end{itemize}

除上述三阶段流程外，针对闭源数据本文还建立了一套标准化的平台屏幕控件标注规范。该规范不仅关注视觉边界（bbox），还显式引入\textbf{控件分类学}、\textbf{控件关系图谱}与\textbf{状态向量}，从而为Referring提供可解释的语义监督信号，并为Grounding提供“可交互目标”的判别依据。

\paragraph{细粒度控件分类学（Taxonomy）}
首先构建了覆盖移动端原子级交互单元与容器级逻辑单元的23类核心控件体系，遵循“\textbf{原子粒度优先}”原则，以减少布局强相关类别造成的歧义。
\begin{itemize}
  \item \textbf{原子交互类（Atomic Interaction）}：BUTTON、ICON、TEXT、IMAGE，以及输入与开关类（TEXT\_INPUT、SWITCH等）。
  \item \textbf{复合容器类（Composite Containers）}：NAVIGATION\_BAR、TAB\_BAR、CONTAINER、POPUP、ADVERTISEMENT、NOTIFICATION等。
  \item \textbf{动态调整类}：SLIDER、PROGRESS\_BAR及其区域类控件等。
\end{itemize}
在该规范中，剔除了CARD\_VIEW、LIST\_ITEM等布局强相关类别，转而采用“原子粒度优先”的原则，以减少模棱两可的布局标注，并促使模型更关注交互本质。

\paragraph{语义关系建模（Directed Relationship Graph）}
传统检测仅预测类别与坐标，缺乏对控件间逻辑的理解。为此，该规范在标注中显式构建控件间的有向关系图（Directed Relationship Graph），该信息对Referring尤为关键：
\begin{itemize}
  \item \textbf{标签--控件绑定（Label-to-Control）}：定义关系$R_{ext}=(T_{label}\rightarrow C_{target})$。例如输入框左侧的“用户名”文本被标记为外部注释，并指向对应的TEXT\_INPUT，使模型能够更稳定地理解“点击输入用户名的位置”这类指令。
  \item \textbf{值--控件绑定（Value-to-Control）}：定义关系$R_{val}=(T_{value}\rightarrow C_{target})$。例如输入框中已填写的“张三”、下拉框当前显示的“北京市”被标记为当前值，并指向父控件，辅助模型区分“输入/修改/清空”等意图。
  \item \textbf{功能依赖（Functional Dependency）}：例如广告弹窗右上角的“关闭（X）”图标与ADVERTISEMENT容器建立依赖关系，使模型学习“如何关闭弹窗/广告”这类意图导航。
\end{itemize}

\paragraph{多维属性与状态向量（Attributes \& State Vectors）}
为支持UI Agent决策，为每个bbox附加多维属性与交互状态信号，核心为交互状态三元组：
\begin{itemize}
  \item \textbf{Activated}：标注开关开启、Tab选中、单选框勾选等状态；
  \item \textbf{Interactable}：区分可交互与置灰不可交互控件；
  \item \textbf{Filled}：标识输入框/选择器是否已有内容，辅助判断执行“输入”还是“修改”。
\end{itemize}
此外，对于ICON类补充\textbf{图标释义}（结合上下文填写象形含义，如“房子$\rightarrow$主页”），并对BUTTON/TEXT\_INPUT记录\textbf{内部文本}，用于增强视觉编码器与文本空间的对齐目标。

\paragraph{人机协同标注与一致性校验（Human-in-the-Loop）}
为保证标注效率与精度，采用了半自动化标注工作流：OCR预处理生成文本bbox与内容，检测模型生成候选控件框，属性预测模型补全类别与状态；标注员进行校验与微调，并引入一致性检查机制（例如外部注释必须有指向目标、关系图无孤立节点、bbox紧致且合法），以降低噪声样本对训练的负面影响。

\section{模型架构与训练策略}
在数据工程基础上，本文进一步从架构与训练两方面优化模型的高分辨率感知与坐标生成能力，以满足GUI小目标定位与中文文本识别的需求。

\subsection{视觉编码器与分辨率自适应：AnyRes机制}
GUI截图包含密集文字与小图标，且移动端屏幕常呈现高长宽比（竖屏）。若直接将原图缩放到视觉编码器的固定正方形输入（如$224\times224$或$336\times336$），会产生不可逆的信息损失：细小图标与字体被压缩后趋于同质化，导致Referring依赖的文本语义对齐变弱、Grounding需要的像素级几何细节丢失，进而表现为定位偏差与相邻控件混淆。

为此，本文采用动态分辨率（AnyRes）机制，其核心思想是同时提供\textbf{全局布局视野}与\textbf{局部高分辨率细节}，并将二者的视觉特征统一送入语言模型。具体流程如下：
\begin{itemize}
  \item \textbf{全局缩放（Global Resize）}：对原始截图生成一张缩放后的全图，用于捕捉导航栏、内容区、弹窗遮挡等全局布局关系，提供稳定的页面结构先验。
  \item \textbf{网格切分（Grid Cropping）}：依据原图长宽比自适应选择网格（如$1\times2$、$2\times2$或$2\times3$），在原始分辨率空间切分得到多个局部Patch；每个Patch再缩放到视觉编码器的合适输入尺寸，使编码器能够清晰感知按钮文字、图标边缘与细粒度状态差异。
  \item \textbf{位置对齐与编码（Patch Position Encoding）}：为避免“看清局部但不知道其在整屏哪里”，为每个Patch附加其网格索引/相对位置编码，使语言模型能够在推理时对齐“局部细节$\leftrightarrow$全局位置”。
  \item \textbf{特征融合（Feature Fusion）}：分别提取全局图与各局部Patch的视觉特征序列，并在Embedding层进行拼接（或按顺序组织后拼接）作为统一的视觉token输入语言模型。这样模型既能利用全局图理解页面结构，又能从局部Patch读取细小文字与图标细节，从而同时提升语义理解与精确定位。
\end{itemize}

AnyRes带来的主要代价是视觉token数量增加，从而提升计算开销。本文在网格数量选择上遵循“\textbf{足够看清关键细节}”与“\textbf{控制推理成本}”之间的折中：在典型竖屏场景下优先采用少量Patch覆盖主要区域，以在可接受的计算预算内显著提升Grounding稳定性与中文OCR相关的Referring准确率。

\subsection{坐标表示与定位Token设计：文本化坐标生成}
为充分利用自回归语言模型的生成能力，本文采用文本化坐标表示，将bbox输出为$[0,1000]$范围的归一化整数序列（如\texttt{<box>250,500,\allowbreak ...</box>}）。同时引入\texttt{<|box\_start|>}、\texttt{<|box\_end|>}、\texttt{<|ref\_start|>}等特殊Token，区分普通文本生成与定位任务生成，降低坐标串与自然语言串的混淆。

其中，\textbf{坐标归一化}的目的在于消除不同设备分辨率与长宽比带来的尺度差异：将像素坐标映射到固定范围后，模型学习到的是“相对位置与相对尺寸”的几何概念，而非对某一固定分辨率的硬编码记忆。这使得同一指令在不同屏幕尺寸（例如$1080\times1920$与$1440\times2560$）下仍能输出一致的定位结果，并便于与数据工程中来自不同来源的数据（开源/闭源/合成）进行统一标注与混合训练。同时，使用整数文本序列输出坐标可避免额外回归头设计，并与语言模型的离散生成范式天然兼容。

\textbf{特殊Token}的引入则承担“结构化约束与可解析性”两项作用：一方面，\texttt{<|box\_start|>}、\texttt{<|box\_end|>}等标记显式划分坐标片段的边界，减少模型将坐标数字与自然语言混写导致的格式错误；另一方面，这类标记使下游系统能够以确定性规则解析模型输出，将bbox作为可执行的动作目标（例如点击/框选），从而降低“输出不可用”带来的失败率。对于训练而言，特殊Token也相当于为定位任务提供了更清晰的任务提示（task delimiter），有助于缓解定位生成与对话生成之间的干扰，提升坐标生成的稳定性与一致性。

\subsection{双阶段混合微调（DMT）：GUI注入与通用对齐}
为兼顾GUI能力与通用对话推理能力，本文采用双阶段混合微调（Dual-stage Mixed Tuning, DMT）策略。该策略的出发点是：当模型同时学习“专用能力”（GUI定位、OCR密集对齐）与“通用能力”（多模态对话、指令遵循与推理）时，简单的多任务混合训练往往会引发两类问题：其一，在数据量较大或任务差异较大时容易出现\textbf{能力冲突}（不同任务目标相互干扰，导致某些能力不升反降）；其二，若采用顺序微调，模型又容易发生\textbf{灾难性遗忘}（后续通用对齐覆盖掉前期学到的GUI定位能力）。DMT通过“先专用、再混合回放”的方式，在两者之间取得平衡。
\begin{itemize}
  \item \textbf{阶段一：GUI领域知识注入（Domain Adaptation）}。训练数据100\%为GUI相关数据（开源清洗数据+闭源精标数据+OCR密集任务），以bbox预测与文本对齐为主；训练中冻结视觉编码器大部分参数，重点微调语言模型部分，使模型快速获得稳定的屏幕元素感知与坐标生成能力。
  \item \textbf{阶段二：通用能力恢复与对齐（General Alignment）}。在保持GUI能力的同时引入通用多模态对话数据（约80--90\%），并回放阶段一的高质量GUI样本（约10--20\%）。通过调节混合比例$k$平衡灾难性遗忘与能力冲突，实践表明保留约15--20\%的GUI回放数据即可稳定维持定位精度。
\end{itemize}

从数据混合角度看，已有研究表明：在\textbf{低资源}条件下，多源数据混合可能带来协同效应；而在\textbf{高资源}条件下，简单扩大混合数据总量更容易触发能力冲突，且性能退化往往更多来自“数据总量与任务差异”的叠加，而非比例本身。因此，阶段二采用“通用为主、GUI少量回放”的设计：一方面继续提升通用对话与推理能力，另一方面用比例$k$的GUI回放样本提供持续的定位与OCR对齐约束，显式缓解遗忘。同时，相比直接多任务学习，DMT更能保留通用能力；相比纯顺序微调，DMT又能通过回放显著降低专用能力遗忘，从而实现GUI专用能力与通用能力的更优折中。

\section{实验与评估}
\subsection{评测任务与指标}
围绕Referring与Grounding两项核心能力，本文采用控件级评测作为主要指标：
\begin{itemize}
  \item \textbf{Referring准确率}：给定目标bbox，模型输出的类别/功能/文本描述与标注答案一致的比例；对于含OCR文本的样本，额外关注关键实体与短语的匹配正确率。
  \item \textbf{Grounding准确率}：给定自然语言指令，模型预测bbox与标注bbox达到预设重叠阈值（如IoU$\ge\tau$）且命中目标控件的比例；同时统计在密集小控件场景下的误差分布（偏移量、误命中率）。
\end{itemize}
评测样本覆盖Top 200应用的高频页面与交互组件，包含中文文本、弹窗浮层与多状态控件等困难模式，以检验模型在真实分辨率与真实布局下的泛化能力。

\subsection{实验结果}
为验证各模块的有效性，实验设计从“数据--架构--训练”三个维度进行对比：
\begin{itemize}
  \item \textbf{数据维度}：开源清洗数据/闭源精标数据/合成数据的不同组合；是否引入负样本、关系图谱与状态向量。
  \item \textbf{架构维度}：固定分辨率输入与AnyRes输入的对比；是否使用文本化坐标与定位专用Token。
  \item \textbf{训练维度}：单阶段SFT与DMT两阶段策略对比；阶段二中GUI回放比例$k$对定位精度与通用对话能力的影响。
\end{itemize}

\subsection{分析与讨论}
综合实验结果可观察到三类主要误差来源：\textbf{（1）极密集区域的近邻混淆}（图标间距过小导致误点相邻控件）、\textbf{（2）文本与控件关系未显式建模时的语义歧义}（如“用户名/手机号”标签与输入框的绑定不稳定）、\textbf{（3）跨状态控件的动作选择偏差}（开关已开启仍重复“打开”）。对应地，本章的数据工程与架构设计提供了针对性缓解：AnyRes提升局部细节分辨率；关系图谱与状态向量降低语义歧义；负样本与工具校验减少“看起来像可点但不可点”的误命中。

\section{本章小结}
本章面向GUI场景提出了一套提升视觉语言模型屏幕理解能力的系统方案。我们首先明确Referring与Grounding两项核心能力及其在GUI领域的瓶颈；随后以多源数据工程为重点，构建了开源清洗、闭源精标与合成数据融合的闭环流水线，并通过分类学、关系图谱与状态向量提供更强语义监督；最后在AnyRes高分辨率架构与DMT双阶段混合微调策略的配合下，实现对细粒度定位与中文语义理解的协同提升，为后续UI Agent的任务规划与执行奠定基础。

