% 第二章：相关工作
\chapter{相关工作}

\section{视觉模型}
视觉模型的发展经历了从“以视觉编码器为核心的表征学习”到“视觉-语言对齐的基础模型”，再到“面向复杂任务的多模态指令跟随与推理”的演进。本文的研究问题（多模态数据合成范式）与后续两条任务线（GUI、世界知识）都建立在这一演进脉络之上，因此本节重点回顾与本文最相关的视觉-语言模型（VLM）与多模态大模型（MLLM）的关键路线。

\subsection{视觉-语言对比学习与通用对齐}
对比学习为视觉与语言建立共享语义空间，是现代视觉-语言预训练的重要基石。CLIP通过海量图文对进行对比学习，使模型获得了强迁移的视觉表示能力，并在零样本分类等任务上表现突出\citep{radford2021learning}。这类方法的意义在于：它把视觉表征从“固定类别监督”转向“语言可描述的开放世界概念”，为后续多模态生成式建模提供了通用语义对齐能力。

\subsection{从VLM到MLLM：融合大语言模型的多模态生成}
随着大语言模型（LLM）能力增强，主流路线逐渐从“检索/匹配式VLM”走向“以LLM为核心的生成式MLLM”，即利用视觉编码器提取视觉token，再由LLM统一完成理解、生成与推理。Flamingo采用少样本条件下的视觉-语言融合架构，展示了在多任务多域上的通用能力\citep{alayrac2022flamingo}。BLIP-2进一步提出“冻结图像编码器+冻结LLM”，通过轻量模块完成跨模态桥接，在保持训练效率的同时获得强泛化\citep{li2023blip2}。

在“指令跟随（instruction following）”范式下，多模态模型通过视觉指令微调（Visual Instruction Tuning）进一步获得更好的对话式交互与任务泛化能力。LLaVA系列工作系统化探索了视觉指令微调的数据构造与训练策略\citep{liu2024visual,liu2023improved}。Qwen-VL则强调更通用的视觉理解与工具化能力，在多类视觉任务与多语言场景下展示了较强表现\citep{bai2023qwenvl}。总体而言，这一阶段的核心变化是：模型能力的瓶颈逐渐从“是否能对齐视觉与语言”转向“是否具备足够高质量、可控结构的监督信号”，这也直接引出本文对多模态数据合成范式的研究动机。

\subsection{视觉推理任务与结构化视觉理解}
除通用图文理解外，图表、文档、截图等“结构化视觉输入”对模型的视觉解析与逻辑推理提出了更高要求。ChartQA提出了包含视觉与逻辑推理的图表问答基准，推动模型从“读图”走向“推断”\citep{masry2022chartqa}。DePlot将图表理解转化为“图到表”的结构化转换，从而以可执行中间表征支撑后续推理\citep{liu2022deplot}；MatCha进一步引入数学推理与图表反渲染等机制增强预训练\citep{liu2022matcha}。Pix2Struct提出将“截图解析”作为预训练任务，强调对UI/网页等截图的结构化理解能力\citep{lee2023pix2struct}。这些工作共同表明：当任务依赖结构化信息（布局、文本、关系）时，仅靠通用caption或粗粒度监督往往不足，需要更强的结构化中间表示与针对性数据构造策略。

\subsection{生成式视觉模型与可控图像合成}
在图像生成方向，扩散模型推动了高分辨率、高保真生成能力的快速发展。以SDXL为代表的高质量扩散模型显著提升了高分辨率图像合成质量\citep{podell2023sdxl}，为“文本$\rightarrow$图像”的逆向数据工程提供了可用工具基础。对于本文而言，生成模型的价值不仅在于生成本身，更在于为多模态数据合成提供可控视觉证据，以弥补真实数据的长尾稀缺与弱对齐问题。

\section{用户图形界面（GUI）}
面向GUI的视觉理解与交互是多模态模型落地的重要方向之一。与自然图像相比，GUI截图具有控件密集、文字细小、布局强结构化与状态易变等特征，使得模型需要同时具备（1）细粒度视觉感知能力（小目标定位、OCR相关细节）；（2）结构化理解能力（控件关系、层级与语义绑定）；（3）可执行输出能力（产生可解析的坐标或动作序列）。这类需求决定了GUI领域往往需要更结构化的数据与预训练任务。

相关研究中，Pix2Struct将截图解析作为预训练任务，强调从截图中恢复结构化表示以提升视觉语言理解能力\citep{lee2023pix2struct}，为“截图类输入”的建模提供了代表性路径。总体来看，GUI相关工作所体现的共同趋势是：模型要在真实界面中可靠工作，必须依赖高分辨率视觉输入、文本信息对齐、以及对布局/关系的显式建模；因此数据侧不仅需要“屏幕截图+描述”，更需要可执行的定位监督与高置信的结构化锚点。这一观察与本文第\ref{chap:gui}章采用的多源融合、结构化标注与质控迭代思路一致。

\section{世界知识}
世界知识（World Knowledge）能力指模型对事实、概念与关系网络的掌握，以及在视觉证据与知识之间建立可验证链接并完成多步推理的能力。对于视觉语言模型而言，世界知识增强的难点通常不在于“语言侧是否存储过知识”，而在于“视觉侧是否能稳定锚定实体实例”，以及“推理链路是否可控、可迁移”。

一方面，推理方法与对齐策略对世界知识问答质量至关重要。链式思维提示（Chain-of-Thought, CoT）表明显式推理过程可以显著提升LLM的复杂推理能力\citep{wei2022chain}；面向指令跟随的对齐研究（如InstructGPT）通过人类反馈强化学习提升模型遵循指令与输出质量\citep{ouyang2022training}；Orca等工作进一步探索从强教师模型的“解释轨迹”中进行渐进式学习\citep{mukherjee2023orca}。这些研究共同指出：当任务需要多步推断与事实一致性时，仅监督最终答案可能诱发“短路映射”或幻觉，过程性监督与可验证约束更利于能力迁移。

另一方面，衡量与改进模型事实性也是世界知识增强的重要组成部分。SimpleQA聚焦短回答场景下的事实性评测，为分析模型在事实正确性上的薄弱点提供了参考\citep{wei2024simpleqa}。在多模态场景中，世界知识能力往往体现为“先识别再调用知识”的链路是否稳定、答案是否唯一可核验。本文第\ref{chap:world_knowledge}章即在这一思路下构造RecQA/KnowQA/FinalQA分解，并通过引入带CoT的FinalQA提升可迁移推理能力。

\section{多模态数据合成}
多模态大模型进入“数据质量成为瓶颈”的阶段后，如何规模化构造高质量、强对齐且具备推理密度的数据成为关键问题。现有工作大体可归纳为三类：更好的caption与对齐、更系统的指令进化与交互扩展、以及利用生成模型进行逆向数据工程。

\subsection{更强caption与视觉-语言对齐数据}
ShareGPT4V强调通过更高质量的caption提升多模态模型训练效果，表明“更好的描述”可以显著改善视觉-语言对齐与下游能力\citep{chen2023sharegpt4v}。这类工作揭示：caption不只是训练目标本身，也可以作为后续过滤、校验与任务构造的中间表征，从而把“图像”转化为可复用、可验证的文本证据。

\subsection{进化式指令构造与代理式数据生成}
在纯语言领域，Self-Instruct提出通过模型自举生成指令数据以实现对齐\citep{wang2023selfinstruct}，并启发了多模态领域对“自动化数据生产流程”的探索。AgentInstruct进一步强调agentic flow，将数据生成组织为多步骤的代理流程\citep{mitra2024agentinstruct}。在多模态方向，MMEvol提出Evol-Instruct以增强多模态指令的多样性与能力覆盖\citep{xu2024mmevol}。在结构化推理任务上，Synthesize Step-by-Step通过工具、模板与LLM组合生成推理型数据，为“可执行/可验证”的数据构造提供了范例\citep{li2024synthesize}。这些方法共同体现出“从单轮生成到流程化生成”的趋势：数据构造需要明确能力目标、提供结构化约束，并引入过滤与迭代机制以控制噪声与幻觉。

\subsection{逆向数据工程：从文本到图像的合成闭环}
除“图$\rightarrow$文”的正向构造外，利用生成模型进行“文$\rightarrow$图”的逆向数据工程可在源头提升对齐度与可控性。SynthVLM系统研究了合成图文数据集的构建策略，通过caption清洗、扩散生成与自动指标筛选等环节提升图文对齐与图像质量\citep{liu2025synthvlm}；而高质量扩散模型（如SDXL）为高分辨率图像生成提供了更强基础\citep{podell2023sdxl}。对于本文所研究的“多模态数据合成范式”而言，正向与逆向两类数据流的结合，使得系统既能覆盖真实分布，又能在长尾与强对齐需求下进行可控补齐。
